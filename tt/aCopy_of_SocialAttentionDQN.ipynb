{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sepDWoBqdRMK"
   },
   "source": [
    "# Training a DQN with social attention on `intersection-v0`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kx8X4s8krNWt",
    "outputId": "979a7391-02ef-4f30-b5f7-a95a67cf62a4"
   },
   "outputs": [],
   "source": [
    "#@title Import requirements\n",
    "\n",
    "# Environment\n",
    "%pip install highway-env\n",
    "import gymnasium as gym\n",
    "\n",
    "# Agent\n",
    "%pip install git+https://github.com/eleurent/rl-agents#egg=rl-agents\n",
    "#from stable_baselines3 import PPO\n",
    "#%load_ext tensorboard\n",
    "\n",
    "# Visualisation utils\n",
    "%pip install moviepy #env = gymnasium.make(\"highway-v0\")\n",
    "\n",
    "%pip install imageio_ffmpeg\n",
    "import sys\n",
    "!%pip install tensorboardx gym pyvirtualdisplay\n",
    "# !%apt-get install -y xvfb ffmpeg\n",
    "!%git clone https://github.com/Farama-Foundation/HighwayEnv.git 2> /dev/null\n",
    "sys.path.insert(0, '/HighwayEnv/scripts/') # -----------------------------------------------------------------\n",
    "# from HighwayEnv.scripts.utils import show_videos\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvOEW00pdHrG"
   },
   "source": [
    "## Training\n",
    "\n",
    "We use a policy architecture based on social attention, see [[Leurent and Mercat, 2019]](https://arxiv.org/abs/1911.12250).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m5OThAMlJkfF"
   },
   "outputs": [],
   "source": [
    "buffer = []   # raccoglie da evaluation.rewards\n",
    "data = []     # raccoglie i gruppi da 5 da buffer\n",
    "std_devs = []\n",
    "\n",
    "n_episodes = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XsXhW2Y556G4"
   },
   "outputs": [],
   "source": [
    "from rl_agents.trainer.evaluation import Evaluation\n",
    "import time\n",
    "import numpy\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class CustomEvaluation(Evaluation):\n",
    "  rewards = []\n",
    "\n",
    "  def run_episodes(self):\n",
    "    for self.episode in range(self.num_episodes):\n",
    "      # Run episode\n",
    "      terminal = False\n",
    "      self.reset(seed=self.episode)\n",
    "      rewards = []\n",
    "      start_time = time.time()\n",
    "      while not terminal:\n",
    "        # Step until a terminal step is reached\n",
    "        reward, terminal = self.step()\n",
    "        rewards.append(reward)\n",
    "\n",
    "          # Catch interruptions\n",
    "        try:\n",
    "          if self.env.unwrapped.done:\n",
    "            break\n",
    "        except AttributeError:\n",
    "          pass\n",
    "\n",
    "      # End of episode\n",
    "      duration = time.time() - start_time\n",
    "      self.after_all_episodes(self.episode, rewards, duration)\n",
    "      self.after_some_episodes(self.episode, rewards)\n",
    "      if self.episode%10 == 0:\n",
    "        #stoppare il training\n",
    "        \n",
    "        pass\n",
    "\n",
    "  def train(self):\n",
    "    self.training = True\n",
    "    if getattr(self.agent, \"batched\", False): \n",
    "        self.run_batched_episodes()\n",
    "    else:\n",
    "        self.run_episodes()\n",
    "    self.close()\n",
    "      \n",
    "  def after_all_episodes(self, episode, rewards, duration):\n",
    "    rewards = numpy.array(rewards)\n",
    "    gamma = self.agent.config.get(\"gamma\", 1)\n",
    "    self.writer.add_scalar('episode/length', len(rewards), episode)\n",
    "    self.writer.add_scalar('episode/total_reward', sum(rewards), episode)\n",
    "    self.writer.add_scalar('episode/return', sum(r*gamma**t for t, r in enumerate(rewards)), episode)\n",
    "    self.rewards.append(sum(r*gamma**t for t, r in enumerate(rewards)))\n",
    "    self.writer.add_scalar('episode/fps', len(rewards) / max(duration, 1e-6), episode)\n",
    "    self.writer.add_histogram('episode/rewards', rewards, episode)\n",
    "    logger.info(\"Episode {} score: {:.1f}\".format(episode, sum(rewards)))\n",
    "\n",
    "  def step(self):\n",
    "      \"\"\"\n",
    "          Plan a sequence of actions according to the agent policy, and step the environment accordingly.\n",
    "      \"\"\"\n",
    "      # Query agent for actions sequence\n",
    "      actions = self.agent.plan(self.observation)\n",
    "\n",
    "      # Forward the actions to the environment viewer\n",
    "      try:\n",
    "          self.env.unwrapped.viewer.set_agent_action_sequence(actions)\n",
    "      except AttributeError:\n",
    "          pass\n",
    "\n",
    "      # Step the environment\n",
    "      previous_observation, action = self.observation, actions[0]\n",
    "      transition = self.wrapped_env.step(action)\n",
    "      self.observation, reward, done, truncated, info = transition\n",
    "      terminal = done or truncated\n",
    "\n",
    "      # Call callback\n",
    "      if self.step_callback_fn is not None:\n",
    "          self.step_callback_fn(self.episode, self.wrapped_env, self.agent, transition, self.writer)\n",
    "\n",
    "      # Record the experience.\n",
    "      try:\n",
    "          self.agent.record(previous_observation, action, reward, self.observation, done, info)\n",
    "      except NotImplementedError:\n",
    "          pass\n",
    "\n",
    "     # Writing actions on a file  ----------------------------------------\n",
    "      with open(\"actions.txt\", \"a\") as file:\n",
    "        file.write(str(action) + \" \")\n",
    "\n",
    "      with open(\"output.txt\", \"a\") as file:  \n",
    "        for obs in self.observation:\n",
    "          for o in obs:\n",
    "            #file.write(\"| \" + str(i)+ \") Coordinates: (\" + str(obs[1]) + \", \" + str(obs[2]) + \") | direction: (\" + str(round(obs[3],1)) + \", \" + str(round(obs[4], 1)) + \") | Seno: \" + str(obs[5]) + \"; Coseno: \" +  str(obs[6]) +\"\\n\")\n",
    "            file.write(str(round(o, 4)) + ' ')\n",
    "          file.write('\\n')\n",
    "\n",
    "      if not actions:\n",
    "          raise Exception(\"The agent did not plan any action\")\n",
    "      \n",
    "      return reward, terminal\n",
    "  \n",
    "  def after_all_episodes(self, episode, rewards, duration):\n",
    "    rewards = numpy.array(rewards)\n",
    "    gamma = self.agent.config.get(\"gamma\", 1)\n",
    "    self.writer.add_scalar('episode/length', len(rewards), episode)\n",
    "    self.writer.add_scalar('episode/total_reward', sum(rewards), episode)\n",
    "    self.writer.add_scalar('episode/return', sum(r*gamma**t for t, r in enumerate(rewards)), episode)\n",
    "    self.writer.add_scalar('episode/fps', len(rewards) / max(duration, 1e-6), episode)\n",
    "    self.writer.add_histogram('episode/rewards', rewards, episode)\n",
    "    logger.info(\"Episode {} score: {:.1f}\".format(episode, sum(rewards)))\n",
    "\n",
    "\n",
    "    with open(\"actions.txt\", \"a\") as file:\n",
    "      file.write(\"\\n\")\n",
    "    with open(\"output.txt\", \"a\") as file:\n",
    "      file.write(\"\\n\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqnGqW6jd1xN"
   },
   "source": [
    "Run tensorboard locally to visualize training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QowKW3ix45ZW"
   },
   "outputs": [],
   "source": [
    "#@title Prepare environment, agent, and evaluation process.\n",
    "\n",
    "NUM_EPISODES = n_episodes  #@param {type: \"integer\"}\n",
    "\n",
    "\n",
    "from rl_agents.agents.common.factory import load_agent, load_environment\n",
    "\n",
    "# Get the environment and agent configurations from the rl-agents repository\n",
    "!%git clone https://github.com/eleurent/rl-agents.git 2> /dev/null\n",
    "%cd rl-agents/scripts/\n",
    "env_config = 'configs/IntersectionEnv/env.json'\n",
    "agent_config = 'configs/IntersectionEnv/agents/DQNAgent/ego_attention_2h.json'\n",
    "\n",
    "env = load_environment(env_config)\n",
    "agent = load_agent(agent_config, env)\n",
    "evaluation = CustomEvaluation(env, agent, num_episodes=NUM_EPISODES, display_env=False, display_agent=False, sim_seed=237)\n",
    "print(f\"Ready to train {agent} on {env}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtK9dtfb0JMF"
   },
   "source": [
    "Start training. This should take about an hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sFVq1gFz42Eg"
   },
   "outputs": [],
   "source": [
    "#%cd ../tt/rl-agents/scripts/\n",
    "with open(\"output.txt\", \"w\") as file:\n",
    "     pass\n",
    "with open(\"actions.txt\", \"w\") as file:\n",
    "     pass\n",
    "\n",
    "evaluation.train()\n",
    "for d in evaluation.rewards:\n",
    "  buffer.append(d)\n",
    "\n",
    "model = evaluation.load_agent_model(\"final\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "obs_data = numpy.loadtxt(\"output.txt\")\n",
    "act_data = numpy.zeros(n_episodes)\n",
    "act_n = numpy.zeros(n_episodes)\n",
    "context_data = [[], []] #[[0][n_ep][numero azione] , [1][n_ep][macchina][informazione]\n",
    "block = 0\n",
    "\n",
    "with open(\"actions.txt\", \"r\") as file:\n",
    "    for r, row in enumerate(file):\n",
    "        matrix_list = []\n",
    "        actions_list = []\n",
    "        for e, el in enumerate(row):\n",
    "            matrix=numpy.zeros((15,7))\n",
    "            if not el == ' ' and not el == '\\n':\n",
    "                actions_list.append(el)\n",
    "                for c in range(15):\n",
    "                    for i in range(7):\n",
    "                        #print(obs_data[c+15*block])\n",
    "                        matrix[c][i] = obs_data[c+15*block][i] \n",
    "                matrix_list.append(matrix)   \n",
    "                block +=1\n",
    "        context_data[0].append(numpy.asarray(actions_list))\n",
    "        context_data[1].append(numpy.asarray(matrix_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lanes_finder(car):                  #cell 3 = vx; cell 4 = vy\n",
    "    if car[1] < 0 and car[2] <0: \n",
    "        # upper left\n",
    "        if car[3] == 0:                      #| if abs(car[4]) > abs(car[3]):\n",
    "            return \"3,1\"\n",
    "        elif car[3] < 0:                     #| if abs(car[3]) >= abs(car[4]):\n",
    "            return \"2,2\"\n",
    "        elif car[4] < 0:\n",
    "            return \"2,3\"  \n",
    "        else:                           \n",
    "            return \"3,4\"\n",
    "\n",
    "    elif car[1] >= 0 and car[2] <0:\n",
    "        # upper right\n",
    "        if car[4] == 0:\n",
    "            return \"4,2\"\n",
    "        elif car[4] < 0:\n",
    "            return \"3,3\"\n",
    "        elif car[3] > 0:\n",
    "            return \"3,4\"  \n",
    "        else:\n",
    "            return \"4,1\"\n",
    "        \n",
    "    elif car[1] >=0 and car[2] >=0:\n",
    "        # lower right\n",
    "        if car[3] == 0:\n",
    "            return \"1,3\"\n",
    "        elif car[3] > 0:\n",
    "            return \"4,4\"\n",
    "        elif car[4] > 0:\n",
    "            return \"4,1\"  \n",
    "        else:\n",
    "            return \"1,2\"\n",
    "\n",
    "    else:\n",
    "        # lower left\n",
    "        if car[4] == 0:\n",
    "            return \"2,4\"\n",
    "        elif car[4] > 0:\n",
    "            return \"1,1\"\n",
    "        elif car[3] < 0:\n",
    "            return \"1,2\"  #\n",
    "        else:\n",
    "            return \"2,3\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"faster2.las\", \"w\") as file: #TODO crea una cartella in cui conservare i batch, se non direttamente qui\n",
    "    file.write(\"limit(1..120).\\nlane(1..4).\\ndistance(1..40).\\n\\n\")\n",
    "\n",
    "    for ep in range(n_episodes): #uno dei 4 episodi\n",
    "        for a, action in enumerate(context_data[0][ep]):  #una delle azioni             \n",
    "            ex_string = \"#pos(id_ep\" + str(ep) + \"_act\"+str(a)+\"@1, {\"\n",
    "            if action == '0':             #slower\n",
    "                ex_string += \"},{faster\"\n",
    "            elif action == '1':           #idle\n",
    "                ex_string += \"},{faster\"\n",
    "            elif action == '2':           #faster\n",
    "                ex_string += \"faster},{\"\n",
    "            ex_string += \"}, {ego(\" + lanes_finder(context_data[1][ep][a][0]) + \").\"\n",
    "\n",
    "            existing_cars = []\n",
    "            for car in context_data[1][ep][a][1:]:\n",
    "                if car[0] == 1:  # car is present\n",
    "                    existing_cars.append(numpy.asarray(car))\n",
    "\n",
    "            distances = numpy.zeros((len(existing_cars))) # distanze di ogni ostacolo dall'agente\n",
    "            if len(existing_cars) == 0:\n",
    "                pass\n",
    "            else:\n",
    "                for c in range(len(existing_cars)):\n",
    "                    distances[c] = int(numpy.sqrt((context_data[1][ep][a][0][1] - existing_cars[c][1])**2 + (context_data[1][ep][a][0][2] - existing_cars[c][2])**2)*100)\n",
    "                ex_string += \" obs_car(\" + str(min(distances)) + \",\"\n",
    "                min_index = numpy.argmin(distances)   \n",
    "\n",
    "                ex_string += lanes_finder(existing_cars[min_index]) + \").\"\n",
    "            ex_string += \"}).\"\n",
    "                \n",
    "            file.write(ex_string + \"\\n\")\n",
    "            ex_string = \"\"\n",
    "\n",
    "    file.write('\\nobs_is_far(Dist, Dl) :- obs_car(Dist, L1, L2), limit(Dl), Dist > Dl. \\nobs_is_crossing :- obs_car(Dist, L1, L2), L1 != L2. \\nsame_lane :- ego(L1a, L2a), obs_car(Dist, L1, L2), L1a = L1, L2a = L2.\\n\\n')\n",
    "    file.write(\"\\nmodeh(faster). \\n#modeb(1, obs_is_far(var(distance),  const(limit))). \\n#modeb(1, obs_is_crossing). \\n#modeb(1, same_lane). \\n#modeb(not same_lane). \\n#maxv(4). \\n\\n\")\n",
    "    file.write(\"#bias(\" + '\"' + \"penalty(1, head(X)) :- in_head(X).\" +'\"' + \").\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P3B4aUwNz2JL"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(len(evaluation.rewards))\n",
    "print(len(buffer))\n",
    "\n",
    "\n",
    "print(numpy.mean(evaluation.rewards), np.std(evaluation.rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38-slCg4ij9W"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plot\n",
    "import pandas as pd\n",
    "\n",
    "for i in range(NUM_EPISODES):  #Mi salvo in ogni cella, gli i-esimi valori del buffer, messi in append, così da avere un array con ogni cella occupata da una quintupla di valori\n",
    "    group = [\n",
    "        buffer[i],\n",
    "        buffer[i + NUM_EPISODES],\n",
    "        buffer[i + 2*NUM_EPISODES],\n",
    "        buffer[i + 3*NUM_EPISODES],\n",
    "        buffer[i + 4*NUM_EPISODES]\n",
    "    ]\n",
    "    data.append(np.mean(group)) # i data sono i mean return\n",
    "    std_devs.append(np.std(group)) #array di deviazioni standard\n",
    "\n",
    "# Calcolo la media e la deviazione standard totali\n",
    "mean = np.mean(data)\n",
    "dev = np.std(data)\n",
    "\n",
    "print(mean, dev)\n",
    "\n",
    "#Calcolo media sul valore bilanciato\n",
    "stabilizer = 4500\n",
    "data = []\n",
    "std_devs = []\n",
    "for i in range(NUM_EPISODES-stabilizer):\n",
    "    group = [\n",
    "        buffer[stabilizer+i],\n",
    "        buffer[stabilizer + i + NUM_EPISODES],\n",
    "        buffer[stabilizer + i + NUM_EPISODES],\n",
    "        buffer[stabilizer + i + NUM_EPISODES],\n",
    "        buffer[stabilizer + i + NUM_EPISODES]\n",
    "    ]\n",
    "    data.append(np.mean(group)) # i data sono i mean return\n",
    "    std_devs.append(np.std(group)) #array di deviazioni standard\n",
    "\n",
    "mean = np.mean(data)\n",
    "dev = np.std(data)\n",
    "print(mean, dev)\n",
    "\n",
    "# Grafico\n",
    "print(\"Grafico originale\")\n",
    "plot.figure(figsize=(16, 6))\n",
    "\n",
    "plot.plot(data, label='Return')\n",
    "plot.fill_between(range(len(data)),\n",
    "                 np.array(data) - np.array(std_devs),\n",
    "                 np.array(data) + np.array(std_devs),\n",
    "                 color='b', alpha=0.2, label='Std Dev')\n",
    "plot.xlabel('Episodes')\n",
    "plot.ylabel('Return')\n",
    "plot.title('Return vs Episodes')\n",
    "plot.legend()\n",
    "plot.show()\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "print(\"\\n\\nGrafico Smoothed\\n\\n\")\n",
    "window = 100\n",
    "\n",
    "mean_smoothed = pd.Series(data).rolling(window=window).mean()\n",
    "std_smoothed = pd.Series(data).rolling(window=window).std()\n",
    "\n",
    "\n",
    "plot.figure(figsize=(16, 6))\n",
    "\n",
    "plot.plot(mean_smoothed, label='Mean Return (Smoothed)', color='blue')\n",
    "plot.fill_between(range(len(mean_smoothed)),\n",
    "                mean_smoothed - std_smoothed,\n",
    "                mean_smoothed + std_smoothed,\n",
    "                color='blue', alpha=0.2, label='Std Dev (Smoothed)')\n",
    "plot.xlabel('Episodes')\n",
    "plot.ylabel('Return')\n",
    "plot.title('Return vs Episodes')\n",
    "plot.legend()\n",
    "plot.show()\n",
    "\n",
    "data.clear()\n",
    "std_devs.clear()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKfvu5uhzCIU"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zpxrq2sQLx48"
   },
   "outputs": [],
   "source": [
    "v_test = []\n",
    "print(env_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gY0rpVYUtRpN"
   },
   "outputs": [],
   "source": [
    "from rl_agents.agents.common.factory import load_agent, load_environment\n",
    "from rl_agents.agents.deep_q_network.graphics import *\n",
    "\n",
    "#@title Run the learned policy for a few episodes.\n",
    "print(env_config)\n",
    "env = load_environment(env_config)\n",
    "env.env.env.config['offscreen_rendering'] = True\n",
    "# env.config[\"offscreen_rendering\"] = True    \n",
    "agent = load_agent(agent_config, env)\n",
    "evaluation = CustomEvaluation(env, agent, num_episodes=100, training=False, recover=True, sim_seed=237)\n",
    "evaluation.test()\n",
    "\n",
    "# show_videos(evaluation.run_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(evaluation.rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n34d4Z_xcFbG"
   },
   "outputs": [],
   "source": [
    "for val in evaluation.rewards[_:len(evaluation.rewards)]:\n",
    "  v_test.append(val)\n",
    "\n",
    "print(len(v_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BG5IET-4MB5L"
   },
   "outputs": [],
   "source": [
    "st_dev=numpy.std(v_test)\n",
    "mean_reward=numpy.mean(v_test)\n",
    "\n",
    "\n",
    "print(f\"Return Medio: {mean_reward}; Standard Deviation: {st_dev}\")\n",
    "\n",
    "# Grafico\n",
    "plot.figure(figsize=(16, 6))\n",
    "\n",
    "plot.plot(v_test, label='Return')\n",
    "plot.fill_between(range(len(v_test)),\n",
    "                 np.array(v_test) - st_dev,\n",
    "                 np.array(v_test) + st_dev,\n",
    "                 color='g', alpha=0.3, label='Std Dev')\n",
    "plot.xlabel('Episodes')\n",
    "plot.ylabel('Return')\n",
    "plot.title('Testing Results')\n",
    "plot.show()\n",
    "\n",
    "# ---------------------------------------\n",
    "print(\"\\n\\nGrafico Smoothed\\n\\n\")\n",
    "window = 100\n",
    "\n",
    "mean_smoothed = pd.Series(v_test).rolling(window=window).mean()\n",
    "std_smoothed = pd.Series(v_test).rolling(window=window).std()\n",
    "\n",
    "\n",
    "plot.figure(figsize=(16, 6))\n",
    "\n",
    "plot.plot(mean_smoothed, label='Mean Return (Smoothed)', color='blue')\n",
    "plot.fill_between(range(len(mean_smoothed)),\n",
    "                mean_smoothed - std_smoothed,\n",
    "                mean_smoothed + std_smoothed,\n",
    "                color='green', alpha=0.2, label='Std Dev (Smoothed)')\n",
    "plot.xlabel('Episodes')\n",
    "plot.ylabel('Return')\n",
    "plot.title('Return vs Episodes')\n",
    "plot.legend()\n",
    "plot.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
