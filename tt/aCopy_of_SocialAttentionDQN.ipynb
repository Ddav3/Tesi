{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sepDWoBqdRMK"
   },
   "source": [
    "# Training a DQN with social attention on `intersection-v0`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kx8X4s8krNWt",
    "outputId": "979a7391-02ef-4f30-b5f7-a95a67cf62a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: highway-env in /home/davidesala/.local/lib/python3.10/site-packages (1.10.1)\n",
      "Requirement already satisfied: gymnasium>=1.0.0a2 in /home/davidesala/.local/lib/python3.10/site-packages (from highway-env) (1.0.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/davidesala/.local/lib/python3.10/site-packages (from highway-env) (0.0.4)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/davidesala/.local/lib/python3.10/site-packages (from highway-env) (1.23.5)\n",
      "Requirement already satisfied: pygame>=2.0.2 in /home/davidesala/.local/lib/python3.10/site-packages (from highway-env) (2.6.1)\n",
      "Requirement already satisfied: matplotlib in /home/davidesala/.local/lib/python3.10/site-packages (from highway-env) (3.9.2)\n",
      "Requirement already satisfied: pandas in /home/davidesala/.local/lib/python3.10/site-packages (from highway-env) (2.2.3)\n",
      "Requirement already satisfied: scipy in /home/davidesala/.local/lib/python3.10/site-packages (from highway-env) (1.14.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/davidesala/.local/lib/python3.10/site-packages (from gymnasium>=1.0.0a2->highway-env) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/davidesala/.local/lib/python3.10/site-packages (from gymnasium>=1.0.0a2->highway-env) (4.12.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/davidesala/.local/lib/python3.10/site-packages (from matplotlib->highway-env) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/davidesala/.local/lib/python3.10/site-packages (from matplotlib->highway-env) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/davidesala/.local/lib/python3.10/site-packages (from matplotlib->highway-env) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/davidesala/.local/lib/python3.10/site-packages (from matplotlib->highway-env) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/davidesala/.local/lib/python3.10/site-packages (from matplotlib->highway-env) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /usr/lib/python3/dist-packages (from matplotlib->highway-env) (9.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib->highway-env) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/davidesala/.local/lib/python3.10/site-packages (from matplotlib->highway-env) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->highway-env) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/davidesala/.local/lib/python3.10/site-packages (from pandas->highway-env) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib->highway-env) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting rl-agents\n",
      "  Cloning https://github.com/eleurent/rl-agents to /tmp/pip-install-h1u4u5pe/rl-agents_4569bd57223547e9bb2ed238844abe07\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/eleurent/rl-agents /tmp/pip-install-h1u4u5pe/rl-agents_4569bd57223547e9bb2ed238844abe07\n",
      "  Resolved https://github.com/eleurent/rl-agents to commit 84df15ea977271e6a4d015f10f9f355f7e866890\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: docopt in /home/davidesala/.local/lib/python3.10/site-packages (from rl-agents) (0.6.2)\n",
      "Requirement already satisfied: gymnasium in /home/davidesala/.local/lib/python3.10/site-packages (from rl-agents) (1.0.0)\n",
      "Requirement already satisfied: matplotlib in /home/davidesala/.local/lib/python3.10/site-packages (from rl-agents) (3.9.2)\n",
      "Requirement already satisfied: numba in /home/davidesala/.local/lib/python3.10/site-packages (from rl-agents) (0.60.0)\n",
      "Requirement already satisfied: numpy in /home/davidesala/.local/lib/python3.10/site-packages (from rl-agents) (1.23.5)\n",
      "Requirement already satisfied: pandas in /home/davidesala/.local/lib/python3.10/site-packages (from rl-agents) (2.2.3)\n",
      "Requirement already satisfied: pygame in /home/davidesala/.local/lib/python3.10/site-packages (from rl-agents) (2.6.1)\n",
      "Requirement already satisfied: scipy in /home/davidesala/.local/lib/python3.10/site-packages (from rl-agents) (1.14.1)\n",
      "Requirement already satisfied: seaborn in /home/davidesala/.local/lib/python3.10/site-packages (from rl-agents) (0.13.2)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from rl-agents) (1.16.0)\n",
      "Requirement already satisfied: tensorboardX in /home/davidesala/.local/lib/python3.10/site-packages (from rl-agents) (2.6.2.2)\n",
      "Requirement already satisfied: torch>=1.2.0 in /home/davidesala/.local/lib/python3.10/site-packages (from rl-agents) (2.5.0)\n",
      "Requirement already satisfied: filelock in /home/davidesala/.local/lib/python3.10/site-packages (from torch>=1.2.0->rl-agents) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/davidesala/.local/lib/python3.10/site-packages (from torch>=1.2.0->rl-agents) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/davidesala/.local/lib/python3.10/site-packages (from torch>=1.2.0->rl-agents) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /home/davidesala/.local/lib/python3.10/site-packages (from torch>=1.2.0->rl-agents) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/davidesala/.local/lib/python3.10/site-packages (from torch>=1.2.0->rl-agents) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/davidesala/.local/lib/python3.10/site-packages (from torch>=1.2.0->rl-agents) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/davidesala/.local/lib/python3.10/site-packages (from torch>=1.2.0->rl-agents) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/davidesala/.local/lib/python3.10/site-packages (from torch>=1.2.0->rl-agents) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/davidesala/.local/lib/python3.10/site-packages (from torch>=1.2.0->rl-agents) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/davidesala/.local/lib/python3.10/site-packages (from torch>=1.2.0->rl-agents) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/davidesala/.local/lib/python3.10/site-packages (from torch>=1.2.0->rl-agents) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/davidesala/.local/lib/python3.10/site-packages (from torch>=1.2.0->rl-agents) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/davidesala/.local/lib/python3.10/site-packages (from torch>=1.2.0->rl-agents) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/davidesala/.local/lib/python3.10/site-packages (from torch>=1.2.0->rl-agents) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/davidesala/.local/lib/python3.10/site-packages (from torch>=1.2.0->rl-agents) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/davidesala/.local/lib/python3.10/site-packages (from torch>=1.2.0->rl-agents) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/davidesala/.local/lib/python3.10/site-packages (from torch>=1.2.0->rl-agents) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/davidesala/.local/lib/python3.10/site-packages (from torch>=1.2.0->rl-agents) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/davidesala/.local/lib/python3.10/site-packages (from torch>=1.2.0->rl-agents) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/davidesala/.local/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.2.0->rl-agents) (1.3.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/davidesala/.local/lib/python3.10/site-packages (from gymnasium->rl-agents) (3.1.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/davidesala/.local/lib/python3.10/site-packages (from gymnasium->rl-agents) (0.0.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/davidesala/.local/lib/python3.10/site-packages (from matplotlib->rl-agents) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/davidesala/.local/lib/python3.10/site-packages (from matplotlib->rl-agents) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/davidesala/.local/lib/python3.10/site-packages (from matplotlib->rl-agents) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/davidesala/.local/lib/python3.10/site-packages (from matplotlib->rl-agents) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/davidesala/.local/lib/python3.10/site-packages (from matplotlib->rl-agents) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /usr/lib/python3/dist-packages (from matplotlib->rl-agents) (9.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib->rl-agents) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/davidesala/.local/lib/python3.10/site-packages (from matplotlib->rl-agents) (2.9.0.post0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /home/davidesala/.local/lib/python3.10/site-packages (from numba->rl-agents) (0.43.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->rl-agents) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/davidesala/.local/lib/python3.10/site-packages (from pandas->rl-agents) (2024.2)\n",
      "Requirement already satisfied: protobuf>=3.20 in /home/davidesala/.local/lib/python3.10/site-packages (from tensorboardX->rl-agents) (5.28.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/davidesala/.local/lib/python3.10/site-packages (from jinja2->torch>=1.2.0->rl-agents) (3.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: moviepy in /home/davidesala/.local/lib/python3.10/site-packages (1.0.3)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in /home/davidesala/.local/lib/python3.10/site-packages (from moviepy) (4.4.2)\n",
      "Requirement already satisfied: proglog<=1.0.0 in /home/davidesala/.local/lib/python3.10/site-packages (from moviepy) (0.1.10)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in /home/davidesala/.local/lib/python3.10/site-packages (from moviepy) (2.32.3)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /home/davidesala/.local/lib/python3.10/site-packages (from moviepy) (4.66.6)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/davidesala/.local/lib/python3.10/site-packages (from moviepy) (1.23.5)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in /home/davidesala/.local/lib/python3.10/site-packages (from moviepy) (2.36.0)\n",
      "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /home/davidesala/.local/lib/python3.10/site-packages (from moviepy) (0.5.1)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /usr/lib/python3/dist-packages (from imageio<3.0,>=2.5->moviepy) (9.0.1)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy) (59.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/davidesala/.local/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3.0,>=2.8.1->moviepy) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2020.6.20)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: imageio_ffmpeg in /home/davidesala/.local/lib/python3.10/site-packages (0.5.1)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from imageio_ffmpeg) (59.6.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "/bin/bash: line 1: fg: no job control\n",
      "/bin/bash: line 1: fg: no job control\n"
     ]
    }
   ],
   "source": [
    "#@title Import requirements\n",
    "\n",
    "# Environment\n",
    "%pip install highway-env\n",
    "import gymnasium as gym\n",
    "\n",
    "# Agent\n",
    "%pip install git+https://github.com/eleurent/rl-agents#egg=rl-agents\n",
    "#from stable_baselines3 import PPO\n",
    "#%load_ext tensorboard\n",
    "\n",
    "# Visualisation utils\n",
    "%pip install moviepy #env = gymnasium.make(\"highway-v0\")\n",
    "\n",
    "%pip install imageio_ffmpeg\n",
    "import sys\n",
    "!%pip install tensorboardx gym pyvirtualdisplay\n",
    "# !%apt-get install -y xvfb ffmpeg\n",
    "!%git clone https://github.com/Farama-Foundation/HighwayEnv.git 2> /dev/null\n",
    "sys.path.insert(0, '/HighwayEnv/scripts/') # -----------------------------------------------------------------\n",
    "# from HighwayEnv.scripts.utils import show_videos\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvOEW00pdHrG"
   },
   "source": [
    "## Training\n",
    "\n",
    "We use a policy architecture based on social attention, see [[Leurent and Mercat, 2019]](https://arxiv.org/abs/1911.12250).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "m5OThAMlJkfF"
   },
   "outputs": [],
   "source": [
    "buffer = []   # raccoglie da evaluation.rewards\n",
    "data = []     # raccoglie i gruppi da 5 da buffer\n",
    "std_devs = []\n",
    "\n",
    "n_episodes = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XsXhW2Y556G4"
   },
   "outputs": [],
   "source": [
    "from rl_agents.trainer.evaluation import Evaluation\n",
    "import time\n",
    "import numpy\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class CustomEvaluation(Evaluation):\n",
    "  rewards = []\n",
    "\n",
    "  def run_episodes(self):\n",
    "    for self.episode in range(self.num_episodes):\n",
    "      # Run episode\n",
    "      terminal = False\n",
    "      self.reset(seed=self.episode)\n",
    "      rewards = []\n",
    "      start_time = time.time()\n",
    "      while not terminal:\n",
    "        # Step until a terminal step is reached\n",
    "        reward, terminal = self.step()\n",
    "        rewards.append(reward)\n",
    "\n",
    "          # Catch interruptions\n",
    "        try:\n",
    "          if self.env.unwrapped.done:\n",
    "            break\n",
    "        except AttributeError:\n",
    "          pass\n",
    "\n",
    "      # End of episode\n",
    "      duration = time.time() - start_time\n",
    "      self.after_all_episodes(self.episode, rewards, duration)\n",
    "      self.after_some_episodes(self.episode, rewards)\n",
    "      if self.episode%10 == 0:\n",
    "        #stoppare il training\n",
    "        \n",
    "        pass\n",
    "\n",
    "  def train(self):\n",
    "    self.training = True\n",
    "    if getattr(self.agent, \"batched\", False): \n",
    "        self.run_batched_episodes()\n",
    "    else:\n",
    "        self.run_episodes()\n",
    "    self.close()\n",
    "      \n",
    "  def after_all_episodes(self, episode, rewards, duration):\n",
    "    rewards = numpy.array(rewards)\n",
    "    gamma = self.agent.config.get(\"gamma\", 1)\n",
    "    self.writer.add_scalar('episode/length', len(rewards), episode)\n",
    "    self.writer.add_scalar('episode/total_reward', sum(rewards), episode)\n",
    "    self.writer.add_scalar('episode/return', sum(r*gamma**t for t, r in enumerate(rewards)), episode)\n",
    "    self.rewards.append(sum(r*gamma**t for t, r in enumerate(rewards)))\n",
    "    self.writer.add_scalar('episode/fps', len(rewards) / max(duration, 1e-6), episode)\n",
    "    self.writer.add_histogram('episode/rewards', rewards, episode)\n",
    "    logger.info(\"Episode {} score: {:.1f}\".format(episode, sum(rewards)))\n",
    "\n",
    "  def step(self):\n",
    "      \"\"\"\n",
    "          Plan a sequence of actions according to the agent policy, and step the environment accordingly.\n",
    "      \"\"\"\n",
    "      # Query agent for actions sequence\n",
    "      actions = self.agent.plan(self.observation)\n",
    "\n",
    "      # Forward the actions to the environment viewer\n",
    "      try:\n",
    "          self.env.unwrapped.viewer.set_agent_action_sequence(actions)\n",
    "      except AttributeError:\n",
    "          pass\n",
    "\n",
    "      # Step the environment\n",
    "      previous_observation, action = self.observation, actions[0]\n",
    "      transition = self.wrapped_env.step(action)\n",
    "      self.observation, reward, done, truncated, info = transition\n",
    "      terminal = done or truncated\n",
    "\n",
    "      # Call callback\n",
    "      if self.step_callback_fn is not None:\n",
    "          self.step_callback_fn(self.episode, self.wrapped_env, self.agent, transition, self.writer)\n",
    "\n",
    "      # Record the experience.\n",
    "      try:\n",
    "          self.agent.record(previous_observation, action, reward, self.observation, done, info)\n",
    "      except NotImplementedError:\n",
    "          pass\n",
    "\n",
    "     # Writing actions on a file  ----------------------------------------\n",
    "      with open(\"actions.txt\", \"a\") as file:\n",
    "        file.write(str(action) + \" \")\n",
    "\n",
    "      with open(\"output.txt\", \"a\") as file:  \n",
    "        for obs in self.observation:\n",
    "          for o in obs:\n",
    "            #file.write(\"| \" + str(i)+ \") Coordinates: (\" + str(obs[1]) + \", \" + str(obs[2]) + \") | direction: (\" + str(round(obs[3],1)) + \", \" + str(round(obs[4], 1)) + \") | Seno: \" + str(obs[5]) + \"; Coseno: \" +  str(obs[6]) +\"\\n\")\n",
    "            file.write(str(round(o, 4)) + ' ')\n",
    "          file.write('\\n')\n",
    "\n",
    "      if not actions:\n",
    "          raise Exception(\"The agent did not plan any action\")\n",
    "      \n",
    "      return reward, terminal\n",
    "  \n",
    "  def after_all_episodes(self, episode, rewards, duration):\n",
    "    rewards = numpy.array(rewards)\n",
    "    gamma = self.agent.config.get(\"gamma\", 1)\n",
    "    self.writer.add_scalar('episode/length', len(rewards), episode)\n",
    "    self.writer.add_scalar('episode/total_reward', sum(rewards), episode)\n",
    "    self.writer.add_scalar('episode/return', sum(r*gamma**t for t, r in enumerate(rewards)), episode)\n",
    "    self.writer.add_scalar('episode/fps', len(rewards) / max(duration, 1e-6), episode)\n",
    "    self.writer.add_histogram('episode/rewards', rewards, episode)\n",
    "    logger.info(\"Episode {} score: {:.1f}\".format(episode, sum(rewards)))\n",
    "\n",
    "\n",
    "    with open(\"actions.txt\", \"a\") as file:\n",
    "      file.write(\"\\n\")\n",
    "    with open(\"output.txt\", \"a\") as file:\n",
    "      file.write(\"\\n\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqnGqW6jd1xN"
   },
   "source": [
    "Run tensorboard locally to visualize training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "QowKW3ix45ZW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: fg: no job control\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Choosing GPU device: 0, memory used: 1152 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'rl-agents/scripts/'\n",
      "/home/davidesala/tirocinio/projects/tt/rl-agents/scripts\n",
      "Ready to train <rl_agents.agents.deep_q_network.pytorch.DQNAgent object at 0x742fb12bad10> on <OrderEnforcing<PassiveEnvChecker<IntersectionEnv<intersection-v0>>>>\n"
     ]
    }
   ],
   "source": [
    "#@title Prepare environment, agent, and evaluation process.\n",
    "\n",
    "NUM_EPISODES = n_episodes  #@param {type: \"integer\"}\n",
    "\n",
    "\n",
    "from rl_agents.agents.common.factory import load_agent, load_environment\n",
    "\n",
    "# Get the environment and agent configurations from the rl-agents repository\n",
    "!%git clone https://github.com/eleurent/rl-agents.git 2> /dev/null\n",
    "%cd rl-agents/scripts/\n",
    "env_config = 'configs/IntersectionEnv/env.json'\n",
    "agent_config = 'configs/IntersectionEnv/agents/DQNAgent/ego_attention_2h.json'\n",
    "\n",
    "env = load_environment(env_config)\n",
    "agent = load_agent(agent_config, env)\n",
    "evaluation = CustomEvaluation(env, agent, num_episodes=NUM_EPISODES, display_env=False, display_agent=False, sim_seed=237)\n",
    "print(f\"Ready to train {agent} on {env}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtK9dtfb0JMF"
   },
   "source": [
    "Start training. This should take about an hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "sFVq1gFz42Eg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [BATCH=1/11]--------------------------------------- \n",
      "[INFO] [BATCH=1/11][run_batched_episodes] #samples=0 \n",
      "[INFO] [BATCH=1/11]--------------------------------------- \n",
      "[INFO] Saved DQNAgent model to out/IntersectionEnv/DQNAgent/run_20250423-173917_4069400/checkpoint-0.tar \n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'processes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactions.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      5\u001b[0m      \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mevaluation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m evaluation\u001b[38;5;241m.\u001b[39mrewards:\n\u001b[1;32m      9\u001b[0m   buffer\u001b[38;5;241m.\u001b[39mappend(d)\n",
      "Cell \u001b[0;32mIn[23], line 43\u001b[0m, in \u001b[0;36mCustomEvaluation.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatched\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m): \n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_batched_episodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_episodes()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/rl_agents/trainer/evaluation.py:218\u001b[0m, in \u001b[0;36mEvaluation.run_batched_episodes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# Prepare workers\u001b[39;00m\n\u001b[1;32m    217\u001b[0m env_config, agent_config \u001b[38;5;241m=\u001b[39m serialize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv), serialize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent)\n\u001b[0;32m--> 218\u001b[0m cpu_processes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprocesses\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m os\u001b[38;5;241m.\u001b[39mcpu_count()\n\u001b[1;32m    219\u001b[0m workers_sample_counts \u001b[38;5;241m=\u001b[39m near_split(batch_size, cpu_processes)\n\u001b[1;32m    220\u001b[0m workers_starts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(np\u001b[38;5;241m.\u001b[39mcumsum(np\u001b[38;5;241m.\u001b[39minsert(workers_sample_counts[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)) \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39msum(batch_sizes[:batch]))\n",
      "\u001b[0;31mKeyError\u001b[0m: 'processes'"
     ]
    }
   ],
   "source": [
    "#%cd ../tt/rl-agents/scripts/\n",
    "with open(\"output.txt\", \"w\") as file:\n",
    "     pass\n",
    "with open(\"actions.txt\", \"w\") as file:\n",
    "     pass\n",
    "\n",
    "evaluation.train()\n",
    "for d in evaluation.rewards:\n",
    "  buffer.append(d)\n",
    "\n",
    "model = evaluation.load_agent_model(\"final\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "obs_data = numpy.loadtxt(\"output.txt\")\n",
    "act_data = numpy.zeros(n_episodes)\n",
    "act_n = numpy.zeros(n_episodes)\n",
    "context_data = [[], []] #[[0][n_ep][numero azione] , [1][n_ep][macchina][informazione]\n",
    "block = 0\n",
    "\n",
    "with open(\"actions.txt\", \"r\") as file:\n",
    "    for r, row in enumerate(file):\n",
    "        matrix_list = []\n",
    "        actions_list = []\n",
    "        for e, el in enumerate(row):\n",
    "            matrix=numpy.zeros((15,7))\n",
    "            if not el == ' ' and not el == '\\n':\n",
    "                actions_list.append(el)\n",
    "                for c in range(15):\n",
    "                    for i in range(7):\n",
    "                        #print(obs_data[c+15*block])\n",
    "                        matrix[c][i] = obs_data[c+15*block][i] \n",
    "                matrix_list.append(matrix)   \n",
    "                block +=1\n",
    "        context_data[0].append(numpy.asarray(actions_list))\n",
    "        context_data[1].append(numpy.asarray(matrix_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lanes_finder(car):                  #cell 3 = vx; cell 4 = vy\n",
    "    if car[1] < 0 and car[2] <0: \n",
    "        # upper left\n",
    "        if car[3] == 0:                      #| if abs(car[4]) > abs(car[3]):\n",
    "            return \"3,1\"\n",
    "        elif car[3] < 0:                     #| if abs(car[3]) >= abs(car[4]):\n",
    "            return \"2,2\"\n",
    "        elif car[4] < 0:\n",
    "            return \"2,3\"  \n",
    "        else:                           \n",
    "            return \"3,4\"\n",
    "\n",
    "    elif car[1] >= 0 and car[2] <0:\n",
    "        # upper right\n",
    "        if car[4] == 0:\n",
    "            return \"4,2\"\n",
    "        elif car[4] < 0:\n",
    "            return \"3,3\"\n",
    "        elif car[3] > 0:\n",
    "            return \"3,4\"  \n",
    "        else:\n",
    "            return \"4,1\"\n",
    "        \n",
    "    elif car[1] >=0 and car[2] >=0:\n",
    "        # lower right\n",
    "        if car[3] == 0:\n",
    "            return \"1,3\"\n",
    "        elif car[3] > 0:\n",
    "            return \"4,4\"\n",
    "        elif car[4] > 0:\n",
    "            return \"4,1\"  \n",
    "        else:\n",
    "            return \"1,2\"\n",
    "\n",
    "    else:\n",
    "        # lower left\n",
    "        if car[4] == 0:\n",
    "            return \"2,4\"\n",
    "        elif car[4] > 0:\n",
    "            return \"1,1\"\n",
    "        elif car[3] < 0:\n",
    "            return \"1,2\"  #\n",
    "        else:\n",
    "            return \"2,3\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"faster2.las\", \"w\") as file: #TODO crea una cartella in cui conservare i batch, se non direttamente qui\n",
    "    file.write(\"limit(1..120).\\nlane(1..4).\\ndistance(1..40).\\n\\n\")\n",
    "\n",
    "    for ep in range(n_episodes): #uno dei 4 episodi\n",
    "        for a, action in enumerate(context_data[0][ep]):  #una delle azioni             \n",
    "            ex_string = \"#pos(id_ep\" + str(ep) + \"_act\"+str(a)+\"@1, {\"\n",
    "            if action == '0':             #slower\n",
    "                ex_string += \"},{faster\"\n",
    "            elif action == '1':           #idle\n",
    "                ex_string += \"},{faster\"\n",
    "            elif action == '2':           #faster\n",
    "                ex_string += \"faster},{\"\n",
    "            ex_string += \"}, {ego(\" + lanes_finder(context_data[1][ep][a][0]) + \").\"\n",
    "\n",
    "            existing_cars = []\n",
    "            for car in context_data[1][ep][a][1:]:\n",
    "                if car[0] == 1:  # car is present\n",
    "                    existing_cars.append(numpy.asarray(car))\n",
    "\n",
    "            distances = numpy.zeros((len(existing_cars))) # distanze di ogni ostacolo dall'agente\n",
    "            if len(existing_cars) == 0:\n",
    "                pass\n",
    "            else:\n",
    "                for c in range(len(existing_cars)):\n",
    "                    distances[c] = int(numpy.sqrt((context_data[1][ep][a][0][1] - existing_cars[c][1])**2 + (context_data[1][ep][a][0][2] - existing_cars[c][2])**2)*100)\n",
    "                ex_string += \" obs_car(\" + str(min(distances)) + \",\"\n",
    "                min_index = numpy.argmin(distances)   \n",
    "\n",
    "                ex_string += lanes_finder(existing_cars[min_index]) + \").\"\n",
    "            ex_string += \"}).\"\n",
    "                \n",
    "            file.write(ex_string + \"\\n\")\n",
    "            ex_string = \"\"\n",
    "\n",
    "    file.write('\\nobs_is_far(Dist, Dl) :- obs_car(Dist, L1, L2), limit(Dl), Dist > Dl. \\nobs_is_crossing :- obs_car(Dist, L1, L2), L1 != L2. \\nsame_lane :- ego(L1a, L2a), obs_car(Dist, L1, L2), L1a = L1, L2a = L2.\\n\\n')\n",
    "    file.write(\"\\nmodeh(faster). \\n#modeb(1, obs_is_far(var(distance),  const(limit))). \\n#modeb(1, obs_is_crossing). \\n#modeb(1, same_lane). \\n#modeb(not same_lane). \\n#maxv(4). \\n\\n\")\n",
    "    file.write(\"#bias(\" + '\"' + \"penalty(1, head(X)) :- in_head(X).\" +'\"' + \").\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "P3B4aUwNz2JL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "nan nan\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(len(evaluation.rewards))\n",
    "print(len(buffer))\n",
    "\n",
    "\n",
    "print(numpy.mean(evaluation.rewards), np.std(evaluation.rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "38-slCg4ij9W"
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_EPISODES):  \u001b[38;5;66;03m#Mi salvo in ogni cella, gli i-esimi valori del buffer, messi in append, così da avere un array con ogni cella occupata da una quintupla di valori\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     group \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m----> 7\u001b[0m         \u001b[43mbuffer\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m,\n\u001b[1;32m      8\u001b[0m         buffer[i \u001b[38;5;241m+\u001b[39m NUM_EPISODES],\n\u001b[1;32m      9\u001b[0m         buffer[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mNUM_EPISODES],\n\u001b[1;32m     10\u001b[0m         buffer[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m3\u001b[39m\u001b[38;5;241m*\u001b[39mNUM_EPISODES],\n\u001b[1;32m     11\u001b[0m         buffer[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m4\u001b[39m\u001b[38;5;241m*\u001b[39mNUM_EPISODES]\n\u001b[1;32m     12\u001b[0m     ]\n\u001b[1;32m     13\u001b[0m     data\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(group)) \u001b[38;5;66;03m# i data sono i mean return\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     std_devs\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mstd(group)) \u001b[38;5;66;03m#array di deviazioni standard\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plot\n",
    "import pandas as pd\n",
    "\n",
    "for i in range(NUM_EPISODES):  #Mi salvo in ogni cella, gli i-esimi valori del buffer, messi in append, così da avere un array con ogni cella occupata da una quintupla di valori\n",
    "    group = [\n",
    "        buffer[i],\n",
    "        buffer[i + NUM_EPISODES],\n",
    "        buffer[i + 2*NUM_EPISODES],\n",
    "        buffer[i + 3*NUM_EPISODES],\n",
    "        buffer[i + 4*NUM_EPISODES]\n",
    "    ]\n",
    "    data.append(np.mean(group)) # i data sono i mean return\n",
    "    std_devs.append(np.std(group)) #array di deviazioni standard\n",
    "\n",
    "# Calcolo la media e la deviazione standard totali\n",
    "mean = np.mean(data)\n",
    "dev = np.std(data)\n",
    "\n",
    "print(mean, dev)\n",
    "\n",
    "#Calcolo media sul valore bilanciato\n",
    "stabilizer = 4500\n",
    "data = []\n",
    "std_devs = []\n",
    "for i in range(NUM_EPISODES-stabilizer):\n",
    "    group = [\n",
    "        buffer[stabilizer+i],\n",
    "        buffer[stabilizer + i + NUM_EPISODES],\n",
    "        buffer[stabilizer + i + NUM_EPISODES],\n",
    "        buffer[stabilizer + i + NUM_EPISODES],\n",
    "        buffer[stabilizer + i + NUM_EPISODES]\n",
    "    ]\n",
    "    data.append(np.mean(group)) # i data sono i mean return\n",
    "    std_devs.append(np.std(group)) #array di deviazioni standard\n",
    "\n",
    "mean = np.mean(data)\n",
    "dev = np.std(data)\n",
    "print(mean, dev)\n",
    "\n",
    "# Grafico\n",
    "print(\"Grafico originale\")\n",
    "plot.figure(figsize=(16, 6))\n",
    "\n",
    "plot.plot(data, label='Return')\n",
    "plot.fill_between(range(len(data)),\n",
    "                 np.array(data) - np.array(std_devs),\n",
    "                 np.array(data) + np.array(std_devs),\n",
    "                 color='b', alpha=0.2, label='Std Dev')\n",
    "plot.xlabel('Episodes')\n",
    "plot.ylabel('Return')\n",
    "plot.title('Return vs Episodes')\n",
    "plot.legend()\n",
    "plot.show()\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "print(\"\\n\\nGrafico Smoothed\\n\\n\")\n",
    "window = 100\n",
    "\n",
    "mean_smoothed = pd.Series(data).rolling(window=window).mean()\n",
    "std_smoothed = pd.Series(data).rolling(window=window).std()\n",
    "\n",
    "\n",
    "plot.figure(figsize=(16, 6))\n",
    "\n",
    "plot.plot(mean_smoothed, label='Mean Return (Smoothed)', color='blue')\n",
    "plot.fill_between(range(len(mean_smoothed)),\n",
    "                mean_smoothed - std_smoothed,\n",
    "                mean_smoothed + std_smoothed,\n",
    "                color='blue', alpha=0.2, label='Std Dev (Smoothed)')\n",
    "plot.xlabel('Episodes')\n",
    "plot.ylabel('Return')\n",
    "plot.title('Return vs Episodes')\n",
    "plot.legend()\n",
    "plot.show()\n",
    "\n",
    "data.clear()\n",
    "std_devs.clear()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKfvu5uhzCIU"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Zpxrq2sQLx48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configs/IntersectionEnv/env.json\n"
     ]
    }
   ],
   "source": [
    "v_test = []\n",
    "print(env_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gY0rpVYUtRpN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Choosing GPU device: 0, memory used: 883 \n",
      "[INFO] Loaded DQNAgent model from out/IntersectionEnv/DQNAgent/saved_models/latest.tar \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configs/IntersectionEnv/env.json\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'RecordVideo' object has no attribute 'video_recorder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m agent \u001b[38;5;241m=\u001b[39m load_agent(agent_config, env)\n\u001b[1;32m     10\u001b[0m evaluation \u001b[38;5;241m=\u001b[39m CustomEvaluation(env, agent, num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, recover\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sim_seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m237\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mevaluation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# show_videos(evaluation.run_directory)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/rl_agents/trainer/evaluation.py:137\u001b[0m, in \u001b[0;36mEvaluation.test\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_episodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[0;32mIn[4], line 21\u001b[0m, in \u001b[0;36mCustomEvaluation.run_episodes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     18\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m terminal:\n\u001b[1;32m     20\u001b[0m   \u001b[38;5;66;03m# Step until a terminal step is reached\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m   reward, terminal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m   rewards\u001b[38;5;241m.\u001b[39mappend(reward)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# Catch interruptions\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 74\u001b[0m, in \u001b[0;36mCustomEvaluation.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Step the environment\u001b[39;00m\n\u001b[1;32m     73\u001b[0m previous_observation, action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation, actions[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 74\u001b[0m transition \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrapped_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m transition\n\u001b[1;32m     76\u001b[0m terminal \u001b[38;5;241m=\u001b[39m done \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/wrappers/common.py:513\u001b[0m, in \u001b[0;36mRecordEpisodeStatistics.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    511\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    512\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment, recording the episode statistics.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 513\u001b[0m     obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_returns \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_lengths \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/core.py:322\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    321\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/wrappers/rendering.py:350\u001b[0m, in \u001b[0;36mRecordVideo.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    348\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    349\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment using action, recording observations if :attr:`self.recording`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 350\u001b[0m     obs, rew, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_id \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_trigger \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_trigger(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_id):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/wrappers/common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/core.py:322\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    321\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/wrappers/common.py:283\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_step_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:207\u001b[0m, in \u001b[0;36menv_step_passive_checker\u001b[0;34m(env, action)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A passive check for the environment step, investigating the returning data then returning the data unchanged.\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# We don't check the action as for some environments then out-of-bounds values can be given\u001b[39;00m\n\u001b[0;32m--> 207\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    209\u001b[0m     result, \u001b[38;5;28mtuple\u001b[39m\n\u001b[1;32m    210\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpects step result to be a tuple, actual type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(result)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/highway_env/envs/intersection_env.py:136\u001b[0m, in \u001b[0;36mIntersectionEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[0;32m--> 136\u001b[0m     obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_vehicles()\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spawn_vehicle(spawn_probability\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspawn_probability\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/highway_env/envs/common/abstract.py:240\u001b[0m, in \u001b[0;36mAbstractEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe road and vehicle must be initialized in the environment implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    237\u001b[0m     )\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_type\u001b[38;5;241m.\u001b[39mobserve()\n\u001b[1;32m    243\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reward(action)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/highway_env/envs/common/abstract.py:280\u001b[0m, in \u001b[0;36mAbstractEnv._simulate\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;66;03m# Automatically render intermediate simulation steps if a viewer has been launched\u001b[39;00m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;66;03m# Ignored if the rendering is done offscreen\u001b[39;00m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    278\u001b[0m         frame \u001b[38;5;241m<\u001b[39m frames \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    279\u001b[0m     ):  \u001b[38;5;66;03m# Last frame will be rendered through env.render() as usual\u001b[39;00m\n\u001b[0;32m--> 280\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_automatic_rendering\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_auto_render \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/highway_env/envs/common/abstract.py:337\u001b[0m, in \u001b[0;36mAbstractEnv._automatic_rendering\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;124;03mAutomatically render the intermediate frames while an action is still ongoing.\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \n\u001b[1;32m    333\u001b[0m \u001b[38;5;124;03mThis allows to render the whole video and not only single steps corresponding to agent decision-making.\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mIf a RecordVideo wrapper has been set, use it to capture intermediate frames.\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_auto_render:\n\u001b[0;32m--> 337\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_record_video_wrapper \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_record_video_wrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvideo_recorder\u001b[49m:\n\u001b[1;32m    338\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_record_video_wrapper\u001b[38;5;241m.\u001b[39mvideo_recorder\u001b[38;5;241m.\u001b[39mcapture_frame()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RecordVideo' object has no attribute 'video_recorder'"
     ]
    }
   ],
   "source": [
    "from rl_agents.agents.common.factory import load_agent, load_environment\n",
    "from rl_agents.agents.deep_q_network.graphics import *\n",
    "\n",
    "#@title Run the learned policy for a few episodes.\n",
    "print(env_config)\n",
    "env = load_environment(env_config)\n",
    "env.env.env.config['offscreen_rendering'] = True\n",
    "# env.config[\"offscreen_rendering\"] = True    \n",
    "agent = load_agent(agent_config, env)\n",
    "evaluation = CustomEvaluation(env, agent, num_episodes=100, training=False, recover=True, sim_seed=237)\n",
    "evaluation.test()\n",
    "\n",
    "# show_videos(evaluation.run_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(evaluation.rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n34d4Z_xcFbG"
   },
   "outputs": [],
   "source": [
    "for val in evaluation.rewards[_:len(evaluation.rewards)]:\n",
    "  v_test.append(val)\n",
    "\n",
    "print(len(v_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BG5IET-4MB5L"
   },
   "outputs": [],
   "source": [
    "st_dev=numpy.std(v_test)\n",
    "mean_reward=numpy.mean(v_test)\n",
    "\n",
    "\n",
    "print(f\"Return Medio: {mean_reward}; Standard Deviation: {st_dev}\")\n",
    "\n",
    "# Grafico\n",
    "plot.figure(figsize=(16, 6))\n",
    "\n",
    "plot.plot(v_test, label='Return')\n",
    "plot.fill_between(range(len(v_test)),\n",
    "                 np.array(v_test) - st_dev,\n",
    "                 np.array(v_test) + st_dev,\n",
    "                 color='g', alpha=0.3, label='Std Dev')\n",
    "plot.xlabel('Episodes')\n",
    "plot.ylabel('Return')\n",
    "plot.title('Testing Results')\n",
    "plot.show()\n",
    "\n",
    "# ---------------------------------------\n",
    "print(\"\\n\\nGrafico Smoothed\\n\\n\")\n",
    "window = 100\n",
    "\n",
    "mean_smoothed = pd.Series(v_test).rolling(window=window).mean()\n",
    "std_smoothed = pd.Series(v_test).rolling(window=window).std()\n",
    "\n",
    "\n",
    "plot.figure(figsize=(16, 6))\n",
    "\n",
    "plot.plot(mean_smoothed, label='Mean Return (Smoothed)', color='blue')\n",
    "plot.fill_between(range(len(mean_smoothed)),\n",
    "                mean_smoothed - std_smoothed,\n",
    "                mean_smoothed + std_smoothed,\n",
    "                color='green', alpha=0.2, label='Std Dev (Smoothed)')\n",
    "plot.xlabel('Episodes')\n",
    "plot.ylabel('Return')\n",
    "plot.title('Return vs Episodes')\n",
    "plot.legend()\n",
    "plot.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
